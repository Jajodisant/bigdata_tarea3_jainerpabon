# **Proyecto Big Data â€“ AnÃ¡lisis de Ventas de Supermercado**

Este proyecto corresponde a la Tarea 3: Procesamiento de Datos con Apache Spark del curso Fundamentos de la Infraestructura de Big Data (UNAD). El objetivo fue analizar un conjunto de datos de ventas de supermercado, aplicando procesamiento batch y streaming mediante Apache Spark y Apache Kafka.

# **ğŸ“‚ Estructura del repositorio**
    bigdata_tarea3_jainerpabon/
  â”‚
  â”œâ”€â”€ capturas/
  â”‚   â”œâ”€â”€ batch_resultado.png
  â”‚   â”œâ”€â”€ spark_ui_environment.png
  â”‚   â”œâ”€â”€ spark_ui_jobs.png
  â”‚   â”œâ”€â”€ kafka_producer_output.png
  â”‚
  â”œâ”€â”€ scripts/
  â”‚   â”œâ”€â”€ batch_supermarket_sales.py
  â”‚   â”œâ”€â”€ kafka_producer.py
  â”‚   â””â”€â”€ kafka_spark_consumer.py
  â”‚
  â”œâ”€â”€ sales.csv
  â”œâ”€â”€ README.md
  â””â”€â”€ .gitignore

# **âš™ï¸ Contenido del proyecto**
## **1ï¸âƒ£ Procesamiento Batch (por lotes)**
**Script:** scripts/batch_supermarket_sales.py
+ Carga el archivo real sales.csv.
+ Realiza limpieza y transformaciÃ³n de datos.
+ Calcula el promedio de ventas por sucursal.
+ Guarda los resultados procesados en la carpeta resultados_batch/.

## **2ï¸âƒ£ Procesamiento en Tiempo Real (Streaming)**
**Scripts:**
+ scripts/kafka_producer.py â†’ genera datos simulados de sensores (ventas, temperatura, humedad, etc.) y los envÃ­a a un topic de Kafka.
+ scripts/kafka_spark_consumer.py â†’ consume los datos en tiempo real, los procesa en ventanas de tiempo de 10 segundos y calcula promedios por sensor.

## **ğŸ“¸ Evidencias incluidas**
Las capturas de ejecuciÃ³n y anÃ¡lisis se encuentran en la carpeta capturas/ e incluyen:
+ EjecuciÃ³n de los scripts.
+ Consola de Spark Streaming mostrando batches procesados.
+ Paneles de Spark: Environment, Jobs, Executors y Streaming Query Statistics.

## **ğŸ§­ Instrucciones para ejecuciÃ³n**
### **Clonar el repositorio:**
+ git clone https://github.com/Jajodisant/bigdata_tarea3_jainerpabon.git
+ cd bigdata_tarea3_jainerpabon/scripts

### **Iniciar servicios de Kafka:**
+ sudo /opt/kafka/bin/zookeeper-server-start.sh /opt/kafka/config/zookeeper.properties &
+ sudo /opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties &

### **Ejecutar el productor de datos simulados:**
+ python3 kafka_producer.py

### **En otra terminal, iniciar el consumidor de Spark Streaming:**
+ spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 kafka_spark_consumer.py

### **Para el anÃ¡lisis batch:**
+ spark-submit batch_supermarket_sales.py

## **ğŸ§¾ DescripciÃ³n del repositorio**
ImplementaciÃ³n prÃ¡ctica de procesamiento batch y streaming con Apache Spark y Kafka, utilizando el dataset Supermarket Sales, para demostrar el flujo completo de 
anÃ¡lisis de datos en entornos de Big Data.

## **ğŸ‘¤ AutorÃ­a**
**Estudiante:** Jainer PabÃ³n
**Tutor:** Jaime Rubiano Llorente
**Curso:** Big Data
**Universidad:** UNAD â€“ Escuela de Ciencias BÃ¡sicas, TecnologÃ­a e IngenierÃ­a






